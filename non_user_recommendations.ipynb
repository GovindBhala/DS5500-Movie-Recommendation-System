{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Recommendations for Non-Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Top rated movies within a filter: weighted average of # of reviews and average ratings - DONE\n",
    "    - Genomes: only display if above X threshold? Or add relevance score into the weighted average?\n",
    "2. Item-Item recommendation if they liked movie X - TO DO \n",
    "\n",
    "UI elements to do:\n",
    "- Biographic information for the actor, director (IMDB) (?)\n",
    "- Add spell check to actor names \n",
    "\n",
    "UI in the future: \n",
    "- Filtering within personalized recommendations (user ID field)\n",
    "- Tab allowing users to input own ratings and get a recommendation out\n",
    "- EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Run:\n",
    "1. Convert notebook to py file\n",
    "    - Run in command line: py -m jupyter nbconvert --to script streamlit_example.ipynb\n",
    "2. Run streamlit app\n",
    "    - Run in command line: streamlit run streamlit_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import operator\n",
    "import streamlit as st\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    ### processed data from recommendation data exploration.ipynb\n",
    "    # movie attributes\n",
    "    df = pd.read_parquet('movies_processed.parq')\n",
    "    # number of and average ratings by movie\n",
    "    movie_ratings = pd.read_parquet('movies_ratings.parq')\n",
    "\n",
    "    ### data to get additional attributes for display\n",
    "    links = pd.read_csv('data/ml-25m/links.csv')\n",
    "    imdb_movies = pd.read_csv('data/imdb/IMDb movies.csv')\n",
    "\n",
    "    ### genome tags for filtering\n",
    "    tags = pd.read_csv('data/ml-25m/genome-tags.csv')\n",
    "    relevance = pd.read_csv('data/ml-25m/genome-scores.csv')\n",
    "    \n",
    "    return df, movie_ratings, links, imdb_movies, tags, relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average Ratings \n",
    "\\# of ratings * average rating    \n",
    "If just use average rating, many movies only reviewed once or twice. Want highly rated, frequently watched movies   \n",
    "   \n",
    "This does weight poorly rated, but frequently watched movies higher place than well rated, infrequently movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weighted_avg(movie_ratings, df):\n",
    "    # calculate weighted average\n",
    "    movie_ratings['weighted_avg'] = movie_ratings.avg * movie_ratings.cnt\n",
    "\n",
    "    # merge with df with movie attributes\n",
    "    # LEFT merge so keep movies with no ratings (weighted avg = 0) -- can still recommend if fit specific filters\n",
    "    df = pd.merge(df, movie_ratings[['movieId', 'weighted_avg', 'cnt', 'avg']], on = 'movieId', how = 'left')\n",
    "\n",
    "    # replace nulls to 0 \n",
    "    # NOT avg: should display missing if missing\n",
    "    for var in ['weighted_avg', 'cnt']:\n",
    "        df[var] = np.where(df[var].isnull(), 0, df[var])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in additional IMDB attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_merge(imdb_movies, links, df):\n",
    "    # standardize IMDB IDs\n",
    "    imdb_movies['imdbId'] = imdb_movies.imdb_title_id.str.split('tt').str[1]\n",
    "    imdb_movies.imdbId = pd.to_numeric(imdb_movies.imdbId)\n",
    "    \n",
    "    x = len(df)\n",
    "    # merge links to identify IMDB movies\n",
    "    df = pd.merge(df, links[['movieId', 'imdbId']], on = 'movieId')\n",
    "    # merge specific IMDB attributes\n",
    "    df = pd.merge(df, imdb_movies[['imdbId', 'description', 'language', 'duration', 'production_company']])\n",
    "    assert x == len(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in Genome Tags\n",
    "Limit to tags with > 75% relevant in a movie    \n",
    "__Extension__: incorporate relevant score into weighted average when decide what to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_merge(tags, relevance, df):\n",
    "    # merge tags and relevance scores\n",
    "    tags = pd.merge(tags, relevance, on = 'tagId')\n",
    "\n",
    "    # limit to tags > 75% relevant to a movie \n",
    "    # get list of relevant tags per movie\n",
    "    tags_relevant = tags[tags.relevance > 0.75].groupby('movieId').tag.apply(list).to_frame()\n",
    "    tags_relevant.columns = ['tags']\n",
    "\n",
    "    # merge with dataframe\n",
    "    # LEFT merge because want to keep movie even if doesn't have any tags\n",
    "    df = pd.merge(df, tags_relevant, left_on = 'movieId', right_index = True, how = 'left')\n",
    "    # replace missing tags to empty list\n",
    "    df.tags = df.tags.apply(lambda d: d if isinstance(d, list) else [])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downcase Actors, Directors so can match user input non case-sensitive  \n",
    "Keep non-downcased version for displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcasing(df):\n",
    "    df['actors_downcased'] = df.actors_lst.apply(lambda row: [i.lower() for i in row])\n",
    "    df['directors_downcased'] = df.director_lst.apply(lambda row: [i.lower() for i in row])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Unique Lists of Filter Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_list_expand(df, var):\n",
    "    \n",
    "    # expand lists such that one entry per row \n",
    "    expanded = df[[var, 'movieId']]\n",
    "    expanded = pd.DataFrame({\n",
    "        col:np.repeat(expanded[col].values, expanded[var].str.len()) for col in expanded.columns.drop(var)}\n",
    "    ).assign(**{var:np.concatenate(expanded[var].values)})[expanded.columns]\n",
    "\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_lists(df):\n",
    "\n",
    "    # language to list\n",
    "    for var in ['language']:\n",
    "        df[var + '_lst'] = df[var].str.split(', ')\n",
    "        df[var + '_lst'] = df[var + '_lst'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    \n",
    "    # unique lists. Sort alphabetically\n",
    "    genres_unique = np.sort(cat_list_expand(df, 'genres_all').genres_all.unique())\n",
    "    actors_unique = np.sort(cat_list_expand(df, 'actors_downcased').actors_downcased.unique())\n",
    "    directors_unique = np.sort(cat_list_expand(df, 'directors_downcased').directors_downcased.unique())\n",
    "    countries_unique  = np.sort(cat_list_expand(df, 'country_lst').country_lst.unique())\n",
    "    language_unique = np.sort(cat_list_expand(df, 'language_lst').language_lst.unique())\n",
    "    tags_unique = np.sort(cat_list_expand(df, 'tags').tags.unique())\n",
    "    \n",
    "    return genres_unique, actors_unique, directors_unique, countries_unique, language_unique, tags_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DataFrame for Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe(df):\n",
    "    \n",
    "    df_display = df.copy()\n",
    "    # strip year out of title\n",
    "    df_display['title'] = df_display.title_eng.str.split('(').str[0]\n",
    "    # drop columns\n",
    "    df_display = df_display.drop(columns = ['movieId', 'title_eng', 'imdbId', 'language'])\n",
    "    # rename + reorder columns\n",
    "    df_display.columns = ['Year', 'Genres', 'Director(s)', 'Actors', 'Filming Countries', 'weighted_avg', 'Number of Ratings', \n",
    "                         'Average Rating', 'Description', 'Duration (Minutes)', 'Production Company', \n",
    "                         'Tags', 'actors_downcased', 'directors_downcased', 'Language(s)', 'Title']\n",
    "    df_display = df_display[['Title', 'Year', 'Description','Duration (Minutes)', 'Genres', 'Actors', 'Director(s)', \n",
    "                             'Production Company', 'Filming Countries', 'Language(s)', 'Tags',\n",
    "                             'Number of Ratings', 'Average Rating', 'weighted_avg', 'actors_downcased', 'directors_downcased']]\n",
    "\n",
    "    return df_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run above functions w. cacheing\n",
    "These functions will not change when user inputs to filtering change, so only run first time app is opened     \n",
    "st.cache not compatabile with jupyter notebook. Comment out when running in jupyter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache\n",
    "def cached_functions():\n",
    "    \n",
    "    # read in data\n",
    "    df, movie_ratings, links, imdb_movies, tags, relevance = load_data()\n",
    "    # calculate weighted average for sorting\n",
    "    df = weighted_avg(movie_ratings, df)\n",
    "    # merge in IMDB metadata and tags \n",
    "    df = imdb_merge(imdb_movies, links, df)\n",
    "    df = genome_merge(tags, relevance, df)\n",
    "    # downcase actors and directors so match user input non case-sensitive (keep regular casing for display) \n",
    "    df = downcasing(df)\n",
    "\n",
    "    # get unique lists of all filter values\n",
    "    genres_unique, actors_unique, directors_unique, countries_unique, language_unique, tags_unique = unique_lists(df)\n",
    "    \n",
    "    # format df for display\n",
    "    df_display = display_dataframe(df)\n",
    "    \n",
    "    return df, df_display, genres_unique, actors_unique, directors_unique, countries_unique, language_unique, tags_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, df_display, genres_unique, actors_unique, directors_unique, countries_unique, language_unique, tags_unique = cached_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display in Streamlit with filter options\n",
    "Display:\n",
    "- Title\n",
    "- Year\n",
    "- Description\n",
    "- Duration\n",
    "- Genres\n",
    "- Actors\n",
    "- Directors\n",
    "- Production Company\n",
    "- Country\n",
    "- Language\n",
    "- Genome Tags\n",
    "- Number of ratings\n",
    "- Average rating    \n",
    "   \n",
    "Filter by:\n",
    "- Genres\n",
    "- Actors\n",
    "- Directors\n",
    "- Country\n",
    "- Language\n",
    "- Genome Tags\n",
    "\n",
    "Default table is highest rated movies without filters    \n",
    "   \n",
    "   \n",
    "Extensions:\n",
    "- AND/OR advanced search option? This might be difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title('Top Rated Movie Recommendations')\n",
    "st.header('Enter desired filters and select \"Display Recommendations\" \\n')\n",
    "st.write('Please note filters use AND logic')\n",
    "st.write('If you wish to see overall top rated movies, select Display Recommendations without any filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user inputs: multiple selection possible per category\n",
    "genre_input = st.multiselect('Select genre(s)', genres_unique)\n",
    "country_input = st.multiselect('Select country(s)', countries_unique)\n",
    "language_input = st.multiselect('Select language(s)', language_unique)\n",
    "tag_input = st.multiselect('Select genome tags(s)', tags_unique)\n",
    "\n",
    "# actors, directors get text inputs\n",
    "# Dropdowns too much for streamlit to handle\n",
    "# allow multiple entires\n",
    "actor_input = st.text_input('Type actor(s) names separated by comma')\n",
    "if actor_input != '':\n",
    "    # downcase input\n",
    "    actor_input = actor_input.lower()\n",
    "    # split into list \n",
    "    actor_input = actor_input.split(', ')\n",
    "    # check valid actor/in dataframe. Else, notify and do not apply filter\n",
    "    for i in actor_input:\n",
    "        if i not in actors_unique:\n",
    "            st.write('Invalid actor', i)\n",
    "            actor_input = []\n",
    "else:\n",
    "    actor_input = []\n",
    "\n",
    "director_input = st.text_input('Type director(s) names separated by comma')\n",
    "if director_input != '':\n",
    "    # downcase input\n",
    "    director_input = director_input.lower()\n",
    "    # split into list\n",
    "    director_input = director_input.split(', ')\n",
    "    # check valid director/in dataframe. Else, notify and do not apply filter\n",
    "    for i in director_input:\n",
    "        if i not in directors_unique:\n",
    "            st.write('Invalid director', i)\n",
    "            director_input = []\n",
    "else:\n",
    "    director_input = []\n",
    "\n",
    "# display recommendations once hit button\n",
    "if st.button('Display Recommendations'):\n",
    "    # filter dataframe\n",
    "    df_filtered = df_display[(df_display.Genres.map(set(genre_input).issubset)) & \n",
    "                             (df_display['Filming Countries'].map(set(country_input).issubset)) &\n",
    "                             (df_display['Language(s)'].map(set(language_input).issubset)) & \n",
    "                             (df_display.Tags.map(set(tag_input).issubset))  & \n",
    "                             (df_display.actors_downcased.map(set(actor_input).issubset)) &\n",
    "                             (df_display.directors_downcased.map(set(director_input).issubset))\n",
    "                            ].sort_values('weighted_avg', ascending = False).head(10).drop(columns = ['weighted_avg',\n",
    "                                                                                                     'actors_downcased', \n",
    "                                                                                                      'directors_downcased'])\n",
    "    # if no valid movies with combination of filters, notify. Else display dataframe\n",
    "    if len(df_filtered) > 0:\n",
    "        st.write(df_filtered)\n",
    "    else:\n",
    "        st.write('Found no movies that match your selections')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
