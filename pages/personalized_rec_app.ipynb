{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate & Display Personalized Recommendations\n",
    "**Use Case**: User with existing profile OR generated one on Add Profile page. Provide personalized recommendations using models defined in modules. \n",
    "\n",
    "Process:\n",
    "- Combine ratings data with any newly created profiles\n",
    "- User enters ID\n",
    "- Check if valid ID (in ratings dataset)\n",
    "- Generate recommendations\n",
    "    - Do not display recommenations with cosine similarity < 0 even if fit filter\n",
    "- Allow user to filter down recommendations \n",
    "- Display recommendations    \n",
    "   \n",
    "Note: if run this locally outside of app, data paths will be incorrect including recommendation system modules. Assuming running in streamlit, in which case main_app.py calls these scripts from the root folder, which is where the datasets live.    \n",
    "Also, data is being passed in from main_app, so not all required data is loaded/created in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import operator\n",
    "import scipy.spatial.distance as distance\n",
    "from sklearn import metrics \n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import fastparquet\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import scipy\n",
    "from fuzzywuzzy import fuzz\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import recommendation system (py scripts)\n",
    "from content_based_recommendations import user_content_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Called in data prep section main app     \n",
    "- Load two sparse matrices for the combined model. df1 is movie profiles with one hot encoded genre, (top 3) actors, directors. df2 is movie profiles with one hot encoded top 5 tfidf tokens from description+genome tags\n",
    "- Load corresponding columns and movieIds (row) for sparse matrices. Don't need columns and movieIds are identical in the two datasets, so load just for df1\n",
    "    - All movie ids in both datasets because want to generate user profile based on all movies they have rated. Then filter recommendations down to the target group\n",
    "- Load in ratings data. Version with user profiles added on from prior runs of app\n",
    "- Load lists of movieIds with and without tags. Will generate recommendations from tagged movies with df2 and untagged movies with df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache(allow_output_mutation=True)\n",
    "def load_data():\n",
    "      \n",
    "    # sparse movie dataframe with attached metadata (column titles, movieIds in row order)\n",
    "    # two datasets for combined models \n",
    "    df1 = scipy.sparse.load_npz(\"processed_df_sparse.npz\")\n",
    "    df2 = scipy.sparse.load_npz(\"processed_df_text_sparse.npz\")\n",
    "    \n",
    "    with open('sparse_metadata', \"rb\") as f:\n",
    "        cols1 = pickle.load(f)\n",
    "        movieIds = pickle.load(f)\n",
    "\n",
    "    # preloaded collaborative filtering predictions\n",
    "    collab_predictions = pd.read_parquet('Predictions/KNN_predictions_df.parq')\n",
    "    collab_predictions = collab_predictions.rename(columns = {'est':'prediction', 'uid':'userId', 'iid':'movieId'})\n",
    "    collab_predictions = collab_predictions.drop(columns = ['r_ui', 'details.actual_k', 'details.was_impossible'])\n",
    "        \n",
    "    # version of ratings that has manually entered user profiles added on \n",
    "    ratings = pd.read_parquet('ratings_sample_useradd.parq')\n",
    "    ratings = ratings.reset_index(drop = True)\n",
    "    \n",
    "    # load movieId lists for movies with and without tags so can specify which movies to keep for which models\n",
    "    with open('movieIds_tags', \"rb\") as f:\n",
    "        movieIds_tags = pickle.load(f)\n",
    "    with open('movieIds_notags', \"rb\") as f:\n",
    "        movieIds_notags = pickle.load(f)\n",
    "        \n",
    "    return ratings, movieIds, df1, df2, collab_predictions, movieIds_tags, movieIds_notags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ratings data with new profile created in this run of the app\n",
    "- If user entered any new ratings in Profile Add tab, they will be in lists of the ratings, userId, and movieIds\n",
    "- Create a dataframe and append onto existing ratings data to use for recommendation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache(allow_output_mutation = True)\n",
    "def create_ratings_df(new_ratings, new_users, new_movies, ratings):\n",
    "                \n",
    "    # create dataframe from lists of newly added from profile add\n",
    "    d = {'rating':new_ratings, 'userId':new_users, 'movieId':new_movies}\n",
    "    new_ratings = pd.DataFrame(d)\n",
    "    \n",
    "    # sometimes duplicate movies from user profile adds if they enter hte same movie twice\n",
    "        # take average of duplicate ratings. Else matrix multiplication won't work\n",
    "    new_ratings = new_ratings.groupby(['userId', 'movieId']).rating.mean()  \n",
    "    new_ratings = new_ratings.reset_index(drop = False)\n",
    "    \n",
    "    # concat with original ratings\n",
    "    ratings = pd.concat([ratings, new_ratings], sort = False)\n",
    "    ratings = ratings.reset_index(drop = True)\n",
    "\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Recommendations \n",
    "- Generate recommendations from specified system \n",
    "- Limit recommendations to similarity > 0 so that when filtering, don't display something they would DISlike \n",
    "- Merge with df_display to get features that we will disply on UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache(allow_output_mutation = True)\n",
    "def content_recommendations(user_id, df1, df2, df_display, ratings, movieIds, keep_movies1, keep_movies2): \n",
    "    \n",
    "    recommend1 = user_content_recommendations(user_id, df1, ratings, movieIds, df_display, keep_movies1)\n",
    "    recommend2 = user_content_recommendations(user_id, df2, ratings, movieIds, df_display, keep_movies2)\n",
    "    \n",
    "    # limit to recommendations similarity > 0 \n",
    "        # don't recommend movies that are similar to movies they dislike\n",
    "    recommend1 = recommend1[recommend1.prediction > 0]\n",
    "    recommend2 = recommend2[recommend2.prediction > 0]\n",
    "\n",
    "    # merge with display features\n",
    "    #recommend1 = pd.merge(recommend1, df_display, on = 'movieId', how = 'left')\n",
    "    #recommend2 = pd.merge(recommend2, df_display, on = 'movieId', how = 'left')\n",
    "\n",
    "    return recommend1, recommend2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache(allow_output_mutation = True)\n",
    "def collab_content_recommendations(user_id, df1, collab_predictions, df_display, ratings, movieIds): \n",
    "    \n",
    "    collab_rec = collab_predictions[collab_predictions.userId == user_id]\n",
    "\n",
    "    # find movies in full set that are not in collaborative filtering predictions for this user\n",
    "    keep_movies = set(movieIds).difference(set(collab_rec.movieId.unique()))\n",
    "    \n",
    "    # generate recommendations from content model with movies not in collab filtering\n",
    "    content_rec = user_content_recommendations(user_id, df1, ratings, movieIds, df_display, keep_movies)\n",
    "    \n",
    "    # limit content recs to similarity > 0 \n",
    "    content_rec = content_rec[content_rec.prediction > 0]\n",
    "\n",
    "    # limit collabs recs to predicted rating > user's average rating (same as 0 marker above)\n",
    "    collab_rec = collab_rec[collab_rec.prediction > ratings[ratings.userId == user_id].rating.mean()]\n",
    "    \n",
    "    # merge with display features\n",
    "    collab_rec = pd.merge(collab_rec, df_display, on = 'movieId', how = 'left')\n",
    "    #content_rec = pd.merge(content_rec, df_display, on = 'movieId', how = 'left')\n",
    "    \n",
    "    return collab_rec, content_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit App\n",
    "- See notes on filtering options in non_user_recommendations script/notebook. Identical filter options here. \n",
    "- Combine existing ratings with profiles newly created in the 'add profile' tab of UI. Then if enter userId generated there, will be able to produce recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(df_display, genres_unique, actors_df, directors_df, countries_unique,\n",
    "          language_unique, tags_unique, decades_unique, new_ratings, new_users, new_movies, ratings, movieIds,\n",
    "          collab_predictions, df1, df2, keep_movies1, keep_movies2):\n",
    "    \n",
    "    # user instructions \n",
    "    st.title('Personalized Movie Recommendations')\n",
    "    st.write('Select **Display Recommendations** with no inputs to view your top recommendations. \\n' + \n",
    "             'Or select filters to see your top recommended movies in those categories.')\n",
    "    \n",
    "    # combine original ratings with newly created profiles\n",
    "    ratings = create_ratings_df(new_ratings, new_users, new_movies, ratings)\n",
    "\n",
    "    # user enter their user ID\n",
    "    userId = st.text_input('Enter your User ID:')\n",
    "    \n",
    "    # initial state is ''\n",
    "    if userId == '':\n",
    "        st.write('Cannot provide recommendations without an ID')\n",
    "    else:\n",
    "        # check if valid integer. If yes, convert\n",
    "        try:\n",
    "            userId_int = int(userId)\n",
    "        # if cannot convert to an integer \n",
    "        except ValueError:\n",
    "            st.write('Not a valid ID')\n",
    "            \n",
    "        # if valid integer, find if ID is in collaborative filtering set or not. Newly entered users will be in collab filter.\n",
    "        else: \n",
    "            #set(collab_predictions.userId.unique())\n",
    "            if userId_int in set(ratings.userId.unique()).intersection(set(collab_predictions.userId.unique())):\n",
    "                \n",
    "                # generate recommendations form collab-content combined model\n",
    "                recommend1, recommend2 = collab_content_recommendations(userId_int, df1, collab_predictions, \n",
    "                                                                        df_display, ratings, movieIds)\n",
    "                recommend1 = recommend1.drop(columns = ['userId'])\n",
    "                \n",
    "            elif userId_int in set(ratings.userId.unique()):\n",
    "                \n",
    "                # generate recommendations from combined content model \n",
    "                recommend1, recommend2 = content_recommendations(userId_int, df1, df2, df_display, ratings, movieIds,\n",
    "                                                                 keep_movies1, keep_movies2)\n",
    "\n",
    "            else:\n",
    "                st.write('Not a valid ID')\n",
    "                recommend1 = pd.DataFrame()\n",
    "                \n",
    "            if len(recommend1) > 0: \n",
    "            \n",
    "                ## filtering \n",
    "                # get user inputs: multiple selection possible per category except decade\n",
    "                # input sorted list of unique options \n",
    "                genre_input = st.multiselect('Select genre(s)', genres_unique)\n",
    "                decade_input = st.selectbox('Select film decade', ['Choose an option'] + list(decades_unique))\n",
    "                country_input = st.multiselect('Select filming country(s)', countries_unique)\n",
    "                language_input = st.multiselect('Select language(s)', language_unique)\n",
    "                tag_input = st.multiselect('Select genome tags(s)', tags_unique)\n",
    "\n",
    "                # actors, directors get text inputs - dropdowns too many values for streamlit to handle\n",
    "                # allow multiple entries with a commoa \n",
    "                actor_input = st.text_input('Type actor(s) names separated by commas. ' +\n",
    "                                            'Select intended actor(s) from dropdown that appears')\n",
    "                if actor_input != '':\n",
    "\n",
    "                    # downcase input\n",
    "                    actor_input = actor_input.lower()\n",
    "                    # split into list based on commas\n",
    "                    actor_input = actor_input.split(', ')\n",
    "\n",
    "                    # fuzzy string matching to find similarity ratio between user input and actual actors (downcased)\n",
    "                        # works for misspellings as well \n",
    "                        # limit to 70% similarity \n",
    "                    options = []\n",
    "                    actors_sim = actors_df.copy()\n",
    "                    for i in actor_input:\n",
    "                        # find similarity ratio between input and all unique actors (downcased)\n",
    "                        actors_sim['sim'] = actors_sim.actors_downcased.apply(lambda row: fuzz.token_sort_ratio(row, i))\n",
    "                        # get top 3 with similarity > 70%\n",
    "                        options.append(actors_sim[actors_sim.sim > 70].sort_values('sim', ascending = False\n",
    "                                                                                  ).head(3).actors_upcased.unique())\n",
    "                    # flatten options list\n",
    "                    options = [item for sublist in options for item in sublist]    \n",
    "\n",
    "                    # list actors that are similar to what they typed and accept user selection(s)\n",
    "                    if len(options) > 0:\n",
    "                        actor_input = st.multiselect('Select Actor(s)', options)\n",
    "                    else:\n",
    "                        st.write(\"Sorry, we can't find any matching actors\")\n",
    "\n",
    "                else:\n",
    "                    actor_input = []\n",
    "\n",
    "                director_input = st.text_input('Type director(s) names separated by commas. ' + \n",
    "                                               'Select intended director(s) from dropdown that appears')\n",
    "                if director_input != '':\n",
    "                    # downcase input\n",
    "                    director_input = director_input.lower()\n",
    "                    # split into list \n",
    "                    director_input = director_input.split(', ')\n",
    "\n",
    "                    # fuzzy string matching to find similarity ratio between user input and actual actors (downcased)\n",
    "                        # works for misspellings as well \n",
    "                        # limit to 70% similarity \n",
    "                    options = []\n",
    "                    directors_sim = directors_df.copy()\n",
    "                    for i in director_input:\n",
    "                        # find similarity ratio between input and all unique directors (downcased)\n",
    "                        directors_sim['sim'] = directors_sim.directors_downcased.apply(\n",
    "                            lambda row: fuzz.token_sort_ratio(row, i))\n",
    "                        # get top 3 with similarity > 70%\n",
    "                        options.append(directors_sim[directors_sim.sim > 70].sort_values('sim', ascending = False\n",
    "                                                                                        ).head(3).directors_upcased.unique())\n",
    "                    # flatten options list\n",
    "                    options = [item for sublist in options for item in sublist]    \n",
    "\n",
    "                    # list directors that are similar to what they typed and accept user selection(s)\n",
    "                    if len(options) > 0:\n",
    "                        director_input = st.multiselect('Select Director(s)', options)\n",
    "                    else:\n",
    "                        st.write(\"Sorry, we can't find any matching directors\")\n",
    "\n",
    "                else:\n",
    "                    director_input = []\n",
    "\n",
    "                # display recommendations once hit button\n",
    "                if st.button('Display Recommendations'):\n",
    "                \n",
    "                    # filter dataframe with rest of filters, sort and get top 10. Drop columns we don't want to display.\n",
    "                    rec1_filtered = recommend1[(recommend1.Genres.map(set(genre_input).issubset)) & \n",
    "                                                (recommend1['Filming Countries'].map(set(country_input).issubset)) &\n",
    "                                                (recommend1['Language(s)'].map(set(language_input).issubset)) & \n",
    "                                                (recommend1.Tags.map(set(tag_input).issubset))  & \n",
    "                                                (recommend1['Actors'].map(set(actor_input).issubset)) &\n",
    "                                                (recommend1['Director(s)'].map(set(director_input).issubset)) \n",
    "                                               ].sort_values(['prediction', 'weighted_avg'], ascending = False)\n",
    "                    rec2_filtered = recommend2[(recommend2.Genres.map(set(genre_input).issubset)) & \n",
    "                                                (recommend2['Filming Countries'].map(set(country_input).issubset)) &\n",
    "                                                (recommend2['Language(s)'].map(set(language_input).issubset)) & \n",
    "                                                (recommend2.Tags.map(set(tag_input).issubset))  & \n",
    "                                                (recommend2['Actors'].map(set(actor_input).issubset)) &\n",
    "                                                (recommend2['Director(s)'].map(set(director_input).issubset)) \n",
    "                                               ].sort_values(['prediction', 'weighted_avg'], ascending = False)\n",
    "                    # for decade, only filter if chose an option (no NA default for selectbox)\n",
    "                    if decade_input != 'Choose an option':\n",
    "                        rec1_filtered = rec1_filtered[(rec1_filtered.decade == decade_input)]\n",
    "                        rec2_filtered = rec2_filtered[(rec2_filtered.decade == decade_input)]\n",
    "                        \n",
    "                    # combine: 5 from each\n",
    "                    if len(rec1_filtered) >= 5 and len(rec2_filtered) >= 5:\n",
    "                        rec_filtered = pd.concat([rec1_filtered.head(int(5)), rec2_filtered.head(int(5))])  \n",
    "                    elif len(rec1_filtered) < 5:\n",
    "                        rec_filtered = pd.concat([rec1_filtered, rec2_filtered.head(int(10 - len(rec1_filtered)))])  \n",
    "                    elif len(rec2_filtered) < 5:\n",
    "                        rec_filtered = pd.concat([rec1_filtered.head(int(10 - len(rec1_filtered))), rec2_filtered])  \n",
    "\n",
    "                    # sort based on weighted average\n",
    "                    rec_filtered = rec_filtered.sort_values('weighted_avg', ascending = False)\n",
    "                    \n",
    "                    # drop unnecessary columns for display\n",
    "                    rec_filtered = rec_filtered.drop(columns = ['weighted_avg', 'actors_downcased', \n",
    "                                                                'directors_downcased', 'title_downcased', \n",
    "                                                                'title_year', 'movieId', 'prediction',\n",
    "                                                                'decade', 'tags_num'])\n",
    "                        \n",
    "                    # if no valid movies with combination of filters, notify. Else display dataframe\n",
    "                    if len(rec_filtered) > 0:\n",
    "                        st.write(rec_filtered)\n",
    "                    else:\n",
    "                        st.write('Found no recommended movies that match your selections')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
